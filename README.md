# RAG Document Q&A System - Assignment 1

##  Assignment Overview
**Assignment 1 Deliverable**: Complete RAG prototype that ingests **10+ technical PDF documents**, stores chunks in ChromaDB, provides natural language Q&A via Streamlit UI, and returns grounded answers with source citations. **Completed within 1-week deadline.**

---

##  Requirements Checklist
| Assignment Requirement | Status |
|----------------------|--------|
| 10+ PDF/MD documents | âœ… **15 technical PDFs ingested** |
| Vector database | âœ… ChromaDB (local) + Pinecone option |
| Natural language interface | âœ… Streamlit UI |
| Source references | âœ… Chunk citations displayed |
| Working prototype | âœ… Endâ€‘toâ€‘end pipeline built |
| Technical writeâ€‘up | âœ… Included |
| Video walkthrough | âœ… Prepared |

---

##  Key Features
- **PDF Ingestion**: 15 technical documents â†’ 500â€‘character chunks with 100â€‘character overlap  
- **Embeddings**: `sentence-transformers/all-MiniLM-L6-v2` (fast + CPU friendly)  
- **Vector Storage**: ChromaDB local persistence  
- **Optional Cloud DB**: Pinecone integration supported  
- **Retrieval**: Cosine similarity search (Topâ€‘K = 5)  
- **Generation**: HuggingFace LLM with grounded prompt  
- **Hallucination Control**: Returns *â€œNot found in documentsâ€* if context missing  
- **UI**: Streamlit app for natural language Q&A  
- **Source Transparency**: Shows document + chunk references  
- **Metrics**:
  - Retrieval time
  - Generation time
  - Total response time
- **Performance**: ~0.5â€“1.5 sec avg response on CPU

---

##  Tech Stack
- Python
- LangChain
- HuggingFace Transformers
- Sentenceâ€‘Transformers
- ChromaDB (local vector store)
- Pinecone (optional cloud vector DB)
- Streamlit (UI)

---

## ğŸ“ Project Structure
```
RAG-Document-QA/
â”‚
â”œâ”€â”€ app.py                 # Streamlit UI for Q&A
â”œâ”€â”€ ingest.py              # PDF ingestion + embeddings + DB storage
â”œâ”€â”€ query.py               # CLI version for testing
â”‚
â”œâ”€â”€ requirements.txt       # Dependencies
â”œâ”€â”€ README.md              # Documentation
â”œâ”€â”€ .gitignore             # Ignore large/local files
â”‚
â”œâ”€â”€ data/                  # Input PDFs (not required in repo)
â”œâ”€â”€ db/                    # Vector DB (auto-created after ingest)
â””â”€â”€ venv/                  # Virtual environment
```

---

## â–¶ï¸ How to Run

### 1ï¸âƒ£ Install dependencies
```
pip install -r requirements.txt
```

### 2ï¸âƒ£ Ingest documents
```
python ingest.py
```

### 3ï¸âƒ£ Run Streamlit UI
```
streamlit run app.py
```

---

##  How RAG Works in This Project
1. PDFs are loaded and split into chunks  
2. Chunks converted into embeddings  
3. Stored in vector database  
4. User asks question  
5. Topâ€‘K similar chunks retrieved  
6. Context sent to LLM  
7. Grounded answer generated with sources  

---

##  Hallucination Handling
- Strict prompt: answer only from context  
- If not found â†’ returns â€œNot found in documentsâ€  
- Source chunks always displayed  

---

##  Future Improvements
- Hybrid search (BM25 + vector)
- Reranking model
- Conversation memory
- Better UI
- Evaluation metrics

---

##  Author
Harish A.N  
RAG Assignment Submission
